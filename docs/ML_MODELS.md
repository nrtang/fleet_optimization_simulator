# Machine Learning Models for Fleet Optimization

This guide explains how to create, train, and use machine learning models with the fleet optimization simulator.

## Table of Contents

1. [Overview](#overview)
2. [Available Models](#available-models)
3. [Quick Start](#quick-start)
4. [Training Models](#training-models)
5. [Using Trained Models](#using-trained-models)
6. [Model Architecture Details](#model-architecture-details)
7. [Best Practices](#best-practices)

## Overview

The fleet optimization simulator supports multiple types of ML models:

- **Neural Network Model** - Supervised learning from expert demonstrations
- **DQN Model** - Reinforcement learning with Deep Q-Networks
- **Ensemble Model** - Combines multiple models for robust predictions

All models inherit from `BaseOptimizationModel` and implement the `make_decisions(state)` method.

## Available Models

### 1. Neural Network Model (Supervised Learning)

**File**: `fleet_optimizer/models/neural_network_model.py`

Learns from expert demonstrations using imitation learning. Trains on data generated by existing models (e.g., greedy model).

**Architecture**:
- Input: 33 features (26 state + 7 vehicle features)
- Hidden layers: 128 → 64 → 32 neurons
- Output: 4 action types (IDLE, PICKUP, CHARGING, REPOSITION)

**Best for**: Learning from existing good policies, fast inference

### 2. DQN Model (Reinforcement Learning)

**File**: `fleet_optimizer/models/dqn_model.py`

Learns optimal policy through trial and error using Q-learning.

**Architecture**:
- Dueling DQN architecture
- Separate value and advantage streams
- Experience replay buffer
- Target network for stable training

**Best for**: Learning from scratch, optimizing for long-term rewards

### 3. Ensemble Model

**File**: `fleet_optimizer/models/ensemble_model.py`

Combines multiple models using voting or weighted aggregation.

**Strategies**:
- **Voting**: Majority vote among models
- **Weighted**: Weighted voting based on model performance
- **Cascade**: Priority-based fallback

**Best for**: Robust predictions, combining strengths of multiple models

## Quick Start

### Step 1: Generate Training Data

First, generate training data using existing models:

```bash
# Generate 100 episodes using greedy model
python scripts/generate_training_data.py \
    --configs configs/default.yaml \
    --episodes 100 \
    --output training_data \
    --format pickle
```

This will create trajectory files in the `training_data` directory.

### Step 2: Train a Neural Network Model

```bash
# Train supervised learning model
python scripts/train_neural_network.py \
    --data training_data \
    --output trained_models/neural_network_model.pt \
    --epochs 50 \
    --batch-size 64 \
    --lr 0.001
```

### Step 3: Train a DQN Model

```bash
# Train reinforcement learning model
python scripts/train_dqn.py \
    --config configs/default.yaml \
    --episodes 500 \
    --output trained_models/dqn_model.pt \
    --lr 0.001 \
    --epsilon 1.0
```

### Step 4: Run Simulation with Trained Model

```bash
# Using neural network
python -m fleet_optimizer simulate configs/neural_network.yaml

# Using DQN
python -m fleet_optimizer simulate configs/dqn.yaml

# Using ensemble
python -m fleet_optimizer simulate configs/ensemble.yaml
```

## Training Models

### Neural Network Training

The neural network learns from expert demonstrations through supervised learning.

**Data Requirements**:
- At least 50-100 episodes of trajectory data
- Generated by a good baseline model (e.g., greedy)

**Training Process**:

```bash
python scripts/train_neural_network.py \
    --data training_data \
    --output trained_models/neural_network_model.pt \
    --epochs 50 \
    --batch-size 64 \
    --lr 0.001 \
    --val-split 0.2
```

**Arguments**:
- `--data`: Directory containing training data
- `--output`: Path to save trained model
- `--epochs`: Number of training epochs
- `--batch-size`: Batch size for training
- `--lr`: Learning rate
- `--val-split`: Validation set ratio

**Output**: Trained model saved as PyTorch checkpoint

**Training Time**: ~5-10 minutes on CPU for 100 episodes

### DQN Training

The DQN learns through reinforcement learning by interacting with the simulator.

**Training Process**:

```bash
python scripts/train_dqn.py \
    --config configs/default.yaml \
    --episodes 500 \
    --output trained_models/dqn_model.pt \
    --lr 0.001 \
    --epsilon 1.0 \
    --epsilon-min 0.01 \
    --epsilon-decay 0.995 \
    --gamma 0.99 \
    --batch-size 64 \
    --buffer-capacity 10000 \
    --target-update-freq 100 \
    --save-freq 100
```

**Arguments**:
- `--config`: Simulation config file
- `--episodes`: Number of training episodes
- `--output`: Path to save trained model
- `--lr`: Learning rate
- `--epsilon`: Initial exploration rate
- `--epsilon-min`: Minimum exploration rate
- `--epsilon-decay`: Exploration decay rate
- `--gamma`: Discount factor (0-1)
- `--batch-size`: Batch size for training
- `--buffer-capacity`: Replay buffer capacity
- `--target-update-freq`: Target network update frequency
- `--save-freq`: Checkpoint save frequency

**Output**: Trained model with checkpoint history

**Training Time**: ~2-4 hours on CPU for 500 episodes

### Monitoring Training

Both training scripts print progress:

```
Epoch 10/50
  Train Loss: 0.4532, Train Acc: 78.32%
  Val Loss: 0.4891, Val Acc: 76.45%
  Saved best model (val_loss: 0.4891)
```

```
Episode 100/500
  Reward: 12450.32
  Avg Reward (100): 11234.56
  Loss: 0.0123
  Epsilon: 0.3659
  Buffer Size: 5432
  Revenue: $15234.50
  Trips: 234
```

## Using Trained Models

### Configuration Files

Each model type has a dedicated config file:

- `configs/neural_network.yaml` - Neural Network model
- `configs/dqn.yaml` - DQN model
- `configs/ensemble.yaml` - Ensemble model

### Neural Network Configuration

```yaml
model:
  type: neural_network
  config:
    model_path: trained_models/neural_network_model.pt
    low_battery_threshold: 0.25
    max_pickup_distance_km: 15.0
    temperature: 1.0  # Sampling temperature
```

### DQN Configuration

```yaml
model:
  type: dqn
  config:
    model_path: trained_models/dqn_model.pt
    training_mode: false  # Set to true for continued training
    epsilon: 0.05  # Low epsilon for inference
    gamma: 0.99
    low_battery_threshold: 0.25
    max_pickup_distance_km: 15.0
```

### Ensemble Configuration

```yaml
model:
  type: ensemble
  config:
    aggregation_strategy: weighted  # voting, weighted, or cascade
    weights: [0.1, 0.5, 0.2, 0.2]  # Model weights
    use_greedy: true
    use_neural_network: true
    use_dqn: true

    # Sub-model configurations
    rule_based_config:
      low_battery_threshold: 0.25
    greedy_config:
      low_battery_threshold: 0.25
      max_pickup_distance_km: 15.0
    neural_network_config:
      model_path: trained_models/neural_network_model.pt
    dqn_config:
      model_path: trained_models/dqn_model.pt
      training_mode: false
```

## Model Architecture Details

### Feature Extraction

All models use the same feature extraction pipeline via `StateFeatureExtractor`.

**State Features (26 total)**:

1. **Temporal (3)**: hour_sin, hour_cos, day_progress
2. **Fleet (12)**: battery levels, vehicle status, utilization
3. **Depot (6)**: capacity, utilization, electricity prices
4. **Request (5)**: pending requests, wait times, urgency

**Vehicle Features (7)**:

1. battery_level
2. is_available
3. is_charging
4. distance_to_nearest_depot
5. distance_to_nearest_request
6. current_lat (normalized)
7. current_lon (normalized)

**Combined Features**: 33 total (26 state + 7 vehicle)

### Action Space

Models output one of 4 actions:

1. **IDLE**: Do nothing
2. **PICKUP_PASSENGER**: Pick up a ride request
3. **CHARGING**: Go to depot for charging
4. **REPOSITION**: Move to a strategic location

### Neural Network Architecture

```
Input (33)
    ↓
Linear(33 → 128) + ReLU + Dropout(0.2)
    ↓
Linear(128 → 64) + ReLU + Dropout(0.2)
    ↓
Linear(64 → 32) + ReLU
    ↓
├─→ Action Head: Linear(32 → 4)      [Action logits]
└─→ Value Head: Linear(32 → 1)       [State value]
```

### DQN Architecture (Dueling)

```
Input (33)
    ↓
Linear(33 → 128) + ReLU + LayerNorm
    ↓
Linear(128 → 64) + ReLU + LayerNorm
    ↓
├─→ Value Stream: Linear(64 → 32 → 1)
└─→ Advantage Stream: Linear(64 → 32 → 4)
    ↓
Q(s,a) = V(s) + (A(s,a) - mean(A(s,a)))
```

## Best Practices

### Data Generation

1. **Diversity**: Generate data with multiple configs for variety
2. **Quantity**: At least 100 episodes for neural network training
3. **Quality**: Use well-performing baseline models (greedy > rule-based)

```bash
# Generate diverse training data
python scripts/generate_training_data.py \
    --configs configs/default.yaml configs/rule_based.yaml \
    --episodes 200 \
    --output training_data
```

### Model Selection

- **Neural Network**: Fast inference, good for production deployment
- **DQN**: Better long-term planning, requires more training time
- **Ensemble**: Most robust, combines strengths of multiple models

### Hyperparameter Tuning

**Neural Network**:
- Learning rate: 0.001 - 0.0001
- Batch size: 32 - 128
- Hidden dim: 64 - 256
- Dropout: 0.1 - 0.3

**DQN**:
- Learning rate: 0.001 - 0.0001
- Gamma: 0.95 - 0.99 (higher = more long-term focus)
- Epsilon decay: 0.99 - 0.999
- Buffer capacity: 5000 - 50000
- Target update: 50 - 200 steps

### Evaluation

Compare models using the built-in metrics:

```bash
# Run simulations with different models
python -m fleet_optimizer simulate configs/neural_network.yaml
python -m fleet_optimizer simulate configs/dqn.yaml
python -m fleet_optimizer simulate configs/ensemble.yaml

# Compare results
# Look at: total_revenue, total_trips, avg_wait_time, utilization
```

**Key Metrics**:
- Total revenue (higher is better)
- Total trips completed (higher is better)
- Average wait time (lower is better)
- Fleet utilization (optimal around 60-80%)
- Empty miles ratio (lower is better)

### Debugging

**Model not loading**:
```python
# Check if file exists
ls -lh trained_models/neural_network_model.pt

# Try loading in Python
import torch
checkpoint = torch.load('trained_models/neural_network_model.pt')
print(checkpoint.keys())
```

**Poor performance**:
1. Check training convergence (loss should decrease)
2. Verify feature extraction is working
3. Compare against baseline models
4. Increase training data/episodes
5. Tune hyperparameters

**Out of memory**:
- Reduce batch size
- Reduce buffer capacity (DQN)
- Use CPU instead of GPU for small models

## Creating Custom Models

To create your own ML model:

### 1. Create Model File

```python
# fleet_optimizer/models/my_model.py

from .base_model import BaseOptimizationModel
from ..utils.feature_extraction import StateFeatureExtractor

class MyModel(BaseOptimizationModel):
    def __init__(self, config: dict = None):
        super().__init__(config)
        self.feature_extractor = StateFeatureExtractor()
        # Initialize your model here

    def make_decisions(self, state: dict) -> list:
        """
        Args:
            state: Contains current_time, fleet, depots,
                   active_requests, distance_func

        Returns:
            List of decision dictionaries
        """
        # Extract features
        features = self.feature_extractor.extract_state_features(state)

        # Your model logic here
        decisions = []

        for vehicle in state['fleet'].vehicles.values():
            if vehicle.available:
                # Predict action for this vehicle
                decision = {
                    'vehicle_id': vehicle.vehicle_id,
                    'action': 'IDLE'  # or other action
                }
                decisions.append(decision)

        return decisions
```

### 2. Register Model

Add to `fleet_optimizer/models/__init__.py`:

```python
from .my_model import MyModel

__all__ = [..., 'MyModel']
```

### 3. Update CLI

Add to `fleet_optimizer/cli.py`:

```python
elif model_type == 'my_model':
    from fleet_optimizer.models import MyModel
    return MyModel(model_config)
```

### 4. Create Config

Create `configs/my_model.yaml`:

```yaml
model:
  type: my_model
  config:
    # Your model parameters
```

### 5. Test

```bash
python -m fleet_optimizer simulate configs/my_model.yaml
```

## Additional Resources

- **Feature Extraction**: `fleet_optimizer/utils/feature_extraction.py`
- **Base Model Interface**: `fleet_optimizer/models/base_model.py`
- **Simulation Engine**: `fleet_optimizer/core/simulation_engine.py`
- **Example Scripts**: `examples/train_ml_model.py`

## Troubleshooting

### Common Issues

**Issue**: Model makes all vehicles IDLE
- **Solution**: Check if model is trained, verify action selection logic

**Issue**: Training loss not decreasing
- **Solution**: Lower learning rate, check feature normalization

**Issue**: DQN explores too much
- **Solution**: Lower epsilon, increase epsilon decay

**Issue**: Ensemble weights not working
- **Solution**: Ensure weights sum to 1.0, check model loading

For more help, open an issue on GitHub or check the documentation.
